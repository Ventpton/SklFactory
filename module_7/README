
Цель этого проекта построить модель, которая сможет классифицировать 10 разных моделей автомобилей.
Для обучение этой модели изначально были предоставлены около 15500 фотографий автомобилей для обучения.
10 Моделей распределены достаточно равномерно по папкам моделей. Окончательная тестирование модели
должно было проходить на совершенно отдельных фотографиях количество которых 6675.

Базовая модель основаная на сети Xception уже давала не плохии результаты в районе 92% правильно
определеных автомобилей. Задача стала как можно больше улучшить этот результат. 

Первое с чего я начал это было изучить предоставленые фотографии. Среди них я нашел какой-то процент
сильно узких фотографий, сильно затемненых, фотографии салонов автомобиля а не внешнего вида. Какие-то фотографии 
я посчитал лучше удалить, какие-то отредоктировать, особенно если на фотографии был рядом автомобиль другой модели
из 10 которых модель должна была классифицировать. Затем я решил добавить около 11000 фотографий для обучения. Фотографии
были взяты с интернет рынка автомобилей. Для аугментации я начал использовать библиотеку albumentations. Эта библиотека 
дает больше возможностей для аугментации картинок.

Я продолжил эксперементировать с сетью Xception и первым делом решил определиться с оптимайзером и learning rate.
Я перебрал несколько оптимайзеров и несколько learning rate и остался на Adam и learning rate 0.0001. Затем я 
я проэксперементировал с методом, который помогает отыскать хороший диапазон learning rate для обучения. Метод
заключается в обучении модели на изменяищемся learning rate, затем по loss графику смотрится на каком диапазоне
learning rate график максимальней уменьшался, тот промежуток learning rate считается хорошим для обучения. Потом была
попытка найденый промежуток learning rate использовать для цикличного обучения модели, но я не был доволен результатом.

Эксперементы со слоями нейросети заняли тоже не мало времени. Вначале я все слои нейросети переучивал, потом попробывал
несколько слоев не переучивать, первые слои нейросети содержут всебе очень базовые признаки, следовательно я их и перестал
переучивать. В моей модели 4 первых не переученых слоя показали лучший результат. Дизайн верхушки нейросети я тоже подобрал
Самый хороший результат я получил с 512 нейронов в слои с регулиризацией l2 и дроп аут рэйт 0.5.

В оптимизаторе Адам я немного проэксперементировал с параметром beta_1, и для обучение нашел что значение 0.87 дало мне
результат получше. Затем я обучил модель основаную на сети Xception на 10 эпохах с фотографиями размером 224 на 224 и 
потом доучил модель на 2 эпохах с фотографиями размером 280 на 280. После этого я решил построить другую модель основаную
на другой сети.

Было проверена несколько сетей и остановил я свой выбор на сети EfficientNetV2M. Она в несколько раз больше Xception и 
поэтому я был лимитирован в размере бэчей. Бэч на обучении состовлял 10. Также было потраченно много времени на эксперементы
с разными параметрами для постройки модели на основе EfficientNetV2M. В конце концов я пришел к модели c верхушкой сети лучшая 
конфигурация для меня это слой с 2048 нейронами и дроп оут рэйт 0.5 , слой с 1024 нейронами и дроп оут рэйт 0.36(регулеризация на обоих слоях l2).
Первые 4 слоя не переучиваются под мою задачу, размер фотографий теперь 320 на 320. Оптимизатор для обучения был подобран Adamax с learning rate
0.0001 на первых 12 эпохах, а затем дообучения на двух эпохах с learning rate 0.00009. Эта модель показала результаты еще лучше чем модель 
построеная мной на основе Xception. 

Последним шагом я решил переучить модель на основе EfficientNetV2M добавив в обучение тест фотографии, которая эта модель и классифицировала
выше.




