
Цель этого проекта построить модель, которая сможет классифицировать 10 разных моделей автомобилей.
Для обучение этой модели изначально были предоставлены около 15500 фотографий автомобилей для обучения.
10 Моделей распределены достаточно равномерно по папкам моделей. Окончательная тестирование модели
должно было проходить на совершенно отдельных фотографиях количество которых 6675.

Базовая модель основанная на сети Xception уже давала не плохие результаты в районе 92% правильно
определённых автомобилей. Задача стала как можно больше улучшить этот результат. 

Первое с чего я начал это было изучить предоставленные фотографии. Среди них я нашел какой-то процент
сильно узких фотографий, сильно затемнённых, фотографии салонов автомобиля, а не внешнего вида. Какие-то фотографии 
я посчитал лучше удалить, какие-то отредактировать, особенно если на фотографии был рядом автомобиль другой модели
из 10 которых модель должна была классифицировать. Затем я решил добавить около 11000 фотографий для обучения. Фотографии
были взяты с интернет рынка автомобилей. Для аугментации я начал использовать библиотеку albumentations. Эта библиотека 
дает больше возможностей для аугментации картинок.

Я продолжил экспериментировать с сетью Xception и первым делом решил определиться с оптимайзером и learning rate.
Я перебрал несколько оптимайзеров и несколько learning rate и остался на Adam и learning rate 0.0001. Затем я 
я поэкспериментировал с методом, который помогает отыскать хороший диапазон learning rate для обучения. Метод
заключается в обучении модели на изменяющемся learning rate, затем по loss графику смотрится на каком диапазоне
learning rate график максимальней уменьшался, тот промежуток learning rate считается хорошим для обучения. Потом была
попытка найденный промежуток learning rate использовать для цикличного обучения модели, но я не был доволен результатом.

Эксперименты со слоями нейросети заняли тоже не мало времени. Вначале я все слои нейросети переучивал, потом попробовал
несколько слоев не переучивать, первые слои нейросети содержат в себе очень базовые признаки, следовательно, я их и перестал
переучивать. В моей модели 4 первых не переученных слоя показали лучший результат. Дизайн верхушки нейросети я тоже подобрал
Самый хороший результат я получил с 512 нейронов в слои с регуляризацией l2 и дроп аут рэйт 0.5.

В оптимизаторе Адам я немного поэкспериментировал с параметром beta_1, и для обучения нашел что значение 0.87 дало мне
результат получше. Затем я обучил модель основанную на сети Xception на 10 эпохах с фотографиями размером 224 на 224 и 
потом доучил модель на 2 эпохах с фотографиями размером 280 на 280. После этого я решил построить другую модель основанную
на другой сети.

Было проверена несколько сетей и остановил я свой выбор на сети EfficientNetV2M. Она в несколько раз больше Xception и 
поэтому я был лимитирован в размере бэчей. Бэч на обучении составлял 10. Также было потрачено много времени на эксперименты
с разными параметрами для постройки модели на основе EfficientNetV2M. В конце концов я пришел к модели c верхушкой сети лучшая 
конфигурация для меня это слой с 2048 нейронами и дроп оут рэйт 0.5 , слой с 1024 нейронами и дроп оут рэйт 0.36(регуляризация на обоих слоях l2).
Первые 4 слоя не переучиваются под мою задачу, размер фотографий теперь 320 на 320. Оптимизатор для обучения был подобран Adamax с learning rate
0.0001 на первых 12 эпохах, а затем дообучения на двух эпохах с learning rate 0.00009. Эта модель показала результаты еще лучше, чем модель 
построенная мной на основе Xception. 

Последним шагом я решил переучить модель на основе EfficientNetV2M добавив в обучение тест фотографии, которая эта модель классифицировала
выше. В этом обучение я использовал почти все фотографии не оставив 0.2 для валидэйшн. 12 эпох обучения на learning rate 0.0001, две эпохи на 
learning rate 0.00009, одна эпоха на learning rate 0.000085. Самый лучший результат полученный 97.632%  



